---
layout: post
title: (Linear Algebra) Notes on Linear Algebra
description:
date: 2020-05-09
categories: [Mathematics]
tag: [Linear Algebra, Matrix]
use_math: true
comments: true
---
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5d4d8e1">1. Linear Equation</a></li>
<li><a href="#org652b42a">2. Linear System</a></li>
<li><a href="#org9d66cf6">3. Identity Matrix</a></li>
<li><a href="#orgc51fa4e">4. Inverse Matrix</a></li>
<li><a href="#org4b7c82d">5. Solving Linear System</a></li>
<li><a href="#orgce9ea3f">6. Linear Combination</a></li>
<li><a href="#orge7710f3">7. Span</a></li>
<li><a href="#org443ac57">8. From Matrix Equation to Vector Equation</a></li>
<li><a href="#org262ba6d">9. Several Perspectives about Matrix Multiplication</a></li>
<li><a href="#org2a46892">10. Linear Independence</a></li>
<li><a href="#orgefe0bad">11. Linear Dependence</a></li>
<li><a href="#org3c662b8">12. Span and Subspace</a></li>
<li><a href="#org5872677">13. Basis of a Subspace</a></li>
<li><a href="#org6537418">14. Dimension of Subspace</a></li>
<li><a href="#org6ddd309">15. Column Space of Matrix</a></li>
<li><a href="#orge03b58b">16. Rank of Matrix</a></li>
<li><a href="#org85695bb">17. Transformation</a></li>
<li><a href="#org54a4274">18. Linear Transformation</a></li>
<li><a href="#org4ceaaf3">19. Transformations between Vectors</a></li>
<li><a href="#org1e65e30">20. Matrix of Linear Transformation</a></li>
<li><a href="#org8eebbea">21. Onto and One-To-One</a></li>
<li><a href="#org1d91d50">22. Least Square</a></li>
<li><a href="#orgba00de3">23. Inner Product</a></li>
<li><a href="#orga0bc1c6">24. Properties of Inner Product</a></li>
<li><a href="#org7ed0b11">25. Vector Norm</a></li>
<li><a href="#org21a4fae">26. Unit Vector</a></li>
<li><a href="#org90d81be">27. Distance between Vectors in $\mathbb{R}^{n}$</a></li>
<li><a href="#org02a935a">28. Inner Product and Angle between Vectors</a></li>
<li><a href="#org135d4b4">29. Orthogonal Vectors</a></li>
<li><a href="#orgfe1ba64">30. Least Square Problem</a></li>
<li><a href="#org0584794">31. Normal Equation</a></li>
<li><a href="#org9d2af8b">32. Another Derivation of Normal Equation</a></li>
<li><a href="#org117f1c8">33. What If $\mathbf{C} =\mathbf{A}^{T}\mathbf{A}$is NOT Invertible?</a></li>
<li><a href="#org5f37e66">34. Orthogonal Projection Perspective</a></li>
<li><a href="#org6f46ee6">35. Orthogonal and Orthonormal Sets</a></li>
<li><a href="#org63d1403">36. Orthogonal and Orthonormal Basis</a></li>
<li><a href="#orgdcb9745">37. Orthogonal Projection $\hat{\mathbf{y}}$of $\mathbf{y}$onto Line</a></li>
<li><a href="#orgc5a61e9">38. Orthogonal Projection $\hat{\mathbf{y}}$of $\mathbf{y}$onto Plane</a></li>
<li><a href="#org434faec">39. Orthogonal Projection when $\mathbf{y} \in W$</a></li>
<li><a href="#org5a42fea">40. Transformation: Orthogonal Projection</a></li>
<li><a href="#orga0ff7ea">41. Orthogonal Projection Perspective</a></li>
<li><a href="#org8d04877">42. Gram-Schmidt Orthogonalization</a></li>
<li><a href="#orgf499216">43. Eigenvectors and Eigenvalues</a></li>
<li><a href="#orgefcb36d">44. Null Space</a></li>
<li><a href="#org82161f1">45. Orthogonal Complement</a></li>
<li><a href="#org3d74862">46. Characteristic Equation</a></li>
<li><a href="#orge17df62">47. Eigenspace</a></li>
<li><a href="#org0237cdc">48. Diagonalization</a></li>
<li><a href="#org2965500">49. Finding $\mathbf{V}$and $\mathbf{D}$</a></li>
<li><a href="#org8f1174e">50. Eigendecomposition</a></li>
<li><a href="#org34c3cd6">51. Linear Transformation via Eigendecomposition</a>
<ul>
<li><a href="#org5a68d84">51.1. Change of Basis</a></li>
<li><a href="#org6a79021">51.2. Element-wise Scaling</a></li>
<li><a href="#orgd6fece8">51.3. Back to Original Basis</a></li>
</ul>
</li>
<li><a href="#org18cafa3">52. Linear Transformation via $\mathbf{A}^{k}$</a></li>
<li><a href="#org75ccd94">53. Geometric Multiplicity and Algebraic Multiplicity</a></li>
<li><a href="#org8f11c7e">54. Singular Value Decomposition</a></li>
<li><a href="#org139aebd">55. SVD as Sum of Outer Products</a></li>
<li><a href="#org8d34ee7">56. Another Perspective of SVD</a></li>
<li><a href="#org1030ef5">57. Computing SVD</a></li>
<li><a href="#orgcc5efd7">58. Diagonalization of Symmetric Matrices</a></li>
<li><a href="#orgc2fa864">59. Spectral Theorem of Symmetric Matrices</a></li>
<li><a href="#orgee73dfb">60. Spectral Decomposition</a></li>
<li><a href="#org49a68a9">61. Positive (Semi-)Definite Matrices</a></li>
<li><a href="#orgc6ee40c">62. Symmetric Positive Definite Matrices</a></li>
<li><a href="#orgab500f3">63. Back to Computing SVD</a></li>
<li><a href="#org98db3a4">64. Eigendecomposition in Machine Learning</a></li>
<li><a href="#orgdc96805">65. Low Rank Approximation of a Matrix</a></li>
<li><a href="#orgc04c417">66. Dimension Reducing Transformation</a></li>
<li><a href="#org0084784">67. Reference</a></li>
</ul>
</div>
</div>

<div id="outline-container-org5d4d8e1" class="outline-2">
<h2 id="org5d4d8e1"><span class="section-number-2">1</span> Linear Equation</h2>
<div class="outline-text-2" id="text-1">
<p>
선형방정식(Linear Equation)은 변수 $x_{1}, &ctdot;, x_{n}$이 있을 때 다음과 같이 작성할 수 있는 방정식을 의미한다.
</p>

\begin{equation}
  \begin{aligned}
    a_{1}x_{1}+a_{2}x_{2}+\cdots+a_{n}x_{n} = b
  \end{aligned}
\end{equation}
<p>
이 때, $b$는 계수를 의미하고 $a_{1},\cdots,a_{n}$값들은 실수 또는 복소수의 미지수를 의미한다. 위 식은 다음과 같이 간결하게 작성할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{a}^{T}\mathbf{x} = b
  \end{aligned}
\end{equation}
<p>
이 때, $\mathbf{a}= \begin{bmatrix}a_{1} \\ \vdots\\ a_{n} \end{bmatrix}$이고 $\mathbf{x}= \begin{bmatrix}x_{1} \\ \vdots\\ x_{n} \end{bmatrix}$이다.
</p>
<hr />
</div>
</div>
<div id="outline-container-org652b42a" class="outline-2">
<h2 id="org652b42a"><span class="section-number-2">2</span> Linear System</h2>
<div class="outline-text-2" id="text-2">
<p>
선형방정식(Linear Equation)의 집합을 선형시스템(Linear System)이라고 한다. n개의 선형방정식 $\mathbf{a}_{1}\mathbf{x}=b_{1}, \cdots, \mathbf{a}_{n}\mathbf{x}=b_{n}$이 있는 경우 이를 다음과 같이 간결하게 선형시스템으로 표현할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{Ax} = \mathbf{b}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org9d66cf6" class="outline-2">
<h2 id="org9d66cf6"><span class="section-number-2">3</span> Identity Matrix</h2>
<div class="outline-text-2" id="text-3">
<p>
항등행렬(Identity Matrix)는 대각성분이 전부 1이고 나머지 성분이 전부 0인 $n\times n$크기의 정방행렬을 의미한다. 일반적으로 $\mathbf{I} \in \mathbb{R}^{n\times n}$으로 표현한다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{I}_{3\times3} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
  \end{aligned}
\end{equation}
<p>
항등행렬에 임의의 벡터 $\mathbf{x}&isin; \mathbb{R}^{n}$울 곱하면 자기 자신이 도출된다.
</p>

\begin{equation}
  \begin{aligned}
    \forall \mathbf{x} \in \mathbb{R}^{n}, \quad \mathbf{I}\mathbf{x} = \mathbf{x}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-orgc51fa4e" class="outline-2">
<h2 id="orgc51fa4e"><span class="section-number-2">4</span> Inverse Matrix</h2>
<div class="outline-text-2" id="text-4">
<p>
정방행렬 $\mathbf{A}&isin; \mathbb{R}^{n &times; n}$에 대한 역행렬(Inverse Matrix) $\mathbf{A}^{-1}$는 다음과 같이 정의된다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} = \mathbf{I}
  \end{aligned}
\end{equation}
<p>
$2\times 2$크기의 정방행렬 $\mathbf{A}= \begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix}$가 있을 때, 역행렬은 다음과 같이 정의된다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A}^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org4b7c82d" class="outline-2">
<h2 id="org4b7c82d"><span class="section-number-2">5</span> Solving Linear System</h2>
<div class="outline-text-2" id="text-5">
<p>
행렬 $\mathbf{A}$의 역행렬이 존재하는 경우 선형시스템은 역행렬을 사용하여 다음과 같이 풀 수 있다.
</p>
\begin{equation}
  \begin{aligned}
    &amp; \mathbf{Ax} = \mathbf{b} \\
    &amp; \mathbf{A}^{-1}\mathbf{Ax} = \mathbf{A}^{-1}\mathbf{b} \\
    &amp; \mathbf{Ix} = \mathbf{A}^{-1}\mathbf{b} \\
    &amp; \mathbf{x} = \mathbf{A}^{-1}\mathbf{b}
  \end{aligned}
\end{equation}
<p>
그러나, 행렬 $\mathbf{A}$의 판별식 $det{\mathbf{A}}=0$인 경우 역행렬이 존재하지 않게되고 위와 같이 문제를 풀 수 없다. <b>이런 경우 선형시스템은 해가 존재하지 않거나 무수히 많은 해가 존재한다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-orgce9ea3f" class="outline-2">
<h2 id="orgce9ea3f"><span class="section-number-2">6</span> Linear Combination</h2>
<div class="outline-text-2" id="text-6">
<p>
여러 벡터 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n} &isin; \mathbb{R}^{n}$이 있을 때 스칼라 값 $c_{1},&ctdot;, c_{n}$에 대하여
</p>

\begin{equation}
  \begin{aligned}
    c_{1}\mathbf{v}_{1} + \cdots + c_{n}\mathbf{v}_{n}
  \end{aligned}
\end{equation}
<p>
을 벡터 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n}$의 가중치 계수 $c_{1},&ctdot;, c_{n}$에 대한  <b>선형결합 (Linear Combination)</b> 이라고 한다. 이 때 가중치 계수 $c_{1},&ctdot;, c_{n}$는 0을 포함한 실수 값을 가진다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orge7710f3" class="outline-2">
<h2 id="orge7710f3"><span class="section-number-2">7</span> Span</h2>
<div class="outline-text-2" id="text-7">
<p>
주어진 여러 벡터 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n} &isin; \mathbb{R}^{n}$에 대해 Span$\{\mathbf{v}_{1},&ctdot;,\mathbf{v}_{n} \}$은 모든 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n}$에 대한 선형결합의 집합을 의미한다. 즉, Span$\{\mathbf{v}_{1},&ctdot;,\mathbf{v}_{n} \}$은 다음과 같이 쓸 수 있는 모든 벡터들의 집합이다.
</p>

\begin{equation}
  \begin{aligned}
    c_{1}\mathbf{v}_{1} + \cdots + c_{n}\mathbf{v}_{n}
  \end{aligned}
\end{equation}
<p>
이는 또한 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n}$에 의해 span된 $\mathbb{R}^{n}$공간 상의 subset이라고도 불린다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org443ac57" class="outline-2">
<h2 id="org443ac57"><span class="section-number-2">8</span> From Matrix Equation to Vector Equation</h2>
<div class="outline-text-2" id="text-8">
<p>
$\mathbf{Ax}= \mathbf{b}$와 같은 선형 시스템을 다음과 같이 열벡터를 기준으로 펼쳐보면
</p>

\begin{equation}
  \begin{aligned}
    \begin{bmatrix} \mathbf{a}_{1} &amp; \mathbf{a}_{2} &amp; \cdots &amp; \mathbf{a}_{n} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} = \mathbf{b}
  \end{aligned}
\end{equation}
<p>
로 나타낼 수 있고 이를 다시 표현하면
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{a}_{1}x_{1} + \mathbf{a}_{2}x_{2} + \cdots + \mathbf{a}_{n}x_{n} = \mathbf{b}
  \end{aligned}
\end{equation}
<p>
와 같이 <b>열벡터들의 선형결합으로 표현</b> 할 수 있게 된다. 만약 $\mathbf{b}$가 Span $\{ \mathbf{a}_{1}, &ctdot;, \mathbf{a}_{n} \}$에 포함되어 있다면 이들의 선형결합으로 표현할 수 있으므로 해가 존재한다. 따라서 $\mathbf{b}&isin; \text{Span}\{ \mathbf{a}_{1}, &ctdot;, \mathbf{a}_{n} \}$일 때 해가 존재한다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org262ba6d" class="outline-2">
<h2 id="org262ba6d"><span class="section-number-2">9</span> Several Perspectives about Matrix Multiplication</h2>
<div class="outline-text-2" id="text-9">
<p>
선형시스템 $\mathbf{Ax}= \mathbf{b}$가 있을 때 이는 곧 $\mathbf{A}$의 열벡터들의 선형결합으로 표현할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{Ax} = [ \mathbf{a}_{1}, \mathbf{a}_{2}, \cdots, \mathbf{a}_{n} ] \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} = \mathbf{a}_{1}x_{1} + \mathbf{a}_{2}x_{2} + \cdots + \mathbf{a}_{n}x_{n} = \mathbf{b}
  \end{aligned}
\end{equation}
<p>
만약 선형시스템에 전치행렬을 적용하여 $\mathbf{x}^{T}\mathbf{A}^{T} = \mathbf{b}^{T}$가 되면
</p>

\begin{equation}
  \begin{aligned}
    \begin{bmatrix} x_{1} &amp; x_{2} &amp; \cdots &amp; x_{n} \end{bmatrix} \begin{bmatrix} \mathbf{a}_{1} \\ \mathbf{a}_{2} \\ \vdots \\ \mathbf{a}_{n} \end{bmatrix} = \mathbf{a}_{1}x_{1} + \mathbf{a}_{2}x_{2} + \cdots + \mathbf{a}_{n}x_{n} =  \mathbf{b}
  \end{aligned}
\end{equation}
<p>
$\mathbf{b}^{T}$는 곧 $\mathbf{A}^{T}$의 행벡터(Row Vector)들의 선형결합으로 표현된다.
</p>

<p>
또한 두 벡터의 곱 $\mathbf{a}\mathbf{b}^{T} = \begin{bmatrix}a_{1} \\ \vdots\\ a_{n} \end{bmatrix}\begin{bmatrix}b_{1} &amp; &ctdot; &amp; b_{n} \end{bmatrix}$의 경우 <b>rank1 outer product</b> 로 볼 수 있다. 즉, $[\mathbf{a}\ \ \mathbf{c}] \begin{bmatrix}\mathbf{b}\\ \mathbf{d}\end{bmatrix}$의 경우 $\mathbf{ab} + \mathbf{cd}$와 같이 벡터곱을 스칼라 곱과 같이 생각할 수 있다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org2a46892" class="outline-2">
<h2 id="org2a46892"><span class="section-number-2">10</span> Linear Independence</h2>
<div class="outline-text-2" id="text-10">
<p>
벡터 집합 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n} &isin; \mathbb{R}^{n}$가 주어졌을 때, 이들 중 부분 벡터들의 집합 $\{ \mathbf{v}_{1}, \mathbf{v}_{2}, &ctdot;, \mathbf{v}_{j-1} \}$이 선형결합을 통해 특정 벡터 $\mathbf{v}_{j},\ j = 1,&ctdot;,n$를 표현할 수 있는지 검사한다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{v}_{j} \in \text{Span} \{ \mathbf{v}_{1}, \mathbf{v}_{2}, \cdots, \mathbf{v}_{j-1}\} \quad \text{for some  } j = 1, \cdots, n? 
  \end{aligned}
\end{equation}
<p>
만약 $\mathbf{v}_{j}$가 선형결합으로 표현이 된다면 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n}$는 <b>선형의존 (Linearly Dependent)</b> 이다. 만약, $\mathbf{v}_{j}$가 표현되지 않는다면 $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n}$는 <b>선형독립 (Linearly Independent)</b> 이다.
</p>

<p>
$\mathbf{Ax}= \mathbf{b}$와 같은 시스템에서 $\mathbf{A}$행렬이 $\mathbb{R}^{n\times m}$이고 $n &lt; m$인 경우 방정식의 개수보다 미지수의 개수가 많으므로 $\mathbf{A}$의 열벡터에 의해 Span되는 공간이 항상 $\mathbf{b}$의 차원보다 크게 되어 <b>무수히 많은 해가 존재한다. 이를 Underdetermined System이라고 한다.</b>
</p>

<p>
만약 $x_{1}\mathbf{v}_{1} + x_{2}\mathbf{v}_{2}+\cdots+x_{n}\mathbf{v}_{n} = \mathbf{0}$같은 동차(homogeneous) 선형방정식이 있다고 하면
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{x} = \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{n} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
  \end{aligned}
\end{equation}
<p>
과 같은 자명해가 존재한다. 이 때, $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n}$가 선형독립이면 자명해 이외에 해는 존재하지 않는다. 하지만, $\mathbf{v}_{1}, &ctdot;, \mathbf{v}_{n}$가 선형의존이면 선형시스템은 자명해 이외에 다른 해가 존재한다.
</p>

<p>
자명해 이외에 다른 해가 존재하는 선형의존(Linearly Dependent) 경우 대해서 생각해보면 예를 들어 $\mathbf{A}$행렬이 다음과 같이 5개의 열을 가진 행렬이라고 했을 때
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A} = \begin{bmatrix} \mathbf{a}_{1} &amp; \mathbf{a}_{2} &amp; \mathbf{a}_{3} &amp; \mathbf{a}_{4} &amp; \mathbf{a}_{5} \end{bmatrix}
  \end{aligned}
\end{equation}
<p>
위 <b>열벡터(Column Vector)들 중 최소한 두 개 이상의 벡터가 선형결합</b> 되어야 동차방정식 $\mathbf{Ax}= \mathbf{0}$의 해를 만족할 수 있다. 예를 들어 $\mathbf{a}_{2}x_{2}$성분이 0이 아닌 경우 이를 다시 영벡터로 만들기 위해서는 다른 1,3,4,5 열벡터들의 선형결합이 $-\mathbf{a}_{2}x_{2}$의 값을 만들어야 한다. 이는 곧 <b>$\mathbf{a}_{2}x_{2}$값을 다른 열벡터들의 선형결합으로 표현할 수 있다는 말과 동치이므로 선형의존인 경우 어떤 하나의 벡터가 다른 벡터들의 선형결합으로 표현될 수 있음을 의미한다.</b> 이를 수식으로 표현하면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    &amp; \mathbf{a}_{j}x_{j} = -\mathbf{a}_{1}x_{1} - \cdots - \mathbf{a}_{j-1}x_{j-1} \\
    &amp; \mathbf{a}_{j} = -\frac{x_{1}}{x_{j}}\mathbf{a}_{1} - \cdots - \frac{x_{j-1}}{x_{j}}\mathbf{a}_{j-1} \in \text{Span}\{ \mathbf{a}_{1}, \mathbf{a}_{2}, \cdots, \mathbf{a}_{j-1} \}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-orgefe0bad" class="outline-2">
<h2 id="orgefe0bad"><span class="section-number-2">11</span> Linear Dependence</h2>
<div class="outline-text-2" id="text-11">
<p>
행렬 $\mathbf{A}$의 열벡터 $\mathbf{a}_{1}, \mathbf{a}_{2}, &ctdot;, \mathbf{a}_{n}$이 <b>선형의존(Linearly Dependent)인 경우 해당 열벡터들은 Span의 차원을 늘리지 않는다.</b> 만약 $\mathbf{A}&isin;\mathbb{R}^{3&times;3}$이고 $\mathbf{a}_{3} &isin; \text{Span}\{ \mathbf{a}_{1}, \mathbf{a}_{2} \}$인 경우 
</p>

\begin{equation}
  \begin{aligned}
    \text{Span}\{ \mathbf{a}_{1}, \mathbf{a}_{2} \} = \text{Span}\{ \mathbf{a}_{1}, \mathbf{a}_{2}, \mathbf{a}_{3} \}
  \end{aligned}
\end{equation}
<p>
만약 $\mathbf{a}_{3} = d_{1}\mathbf{a}_{1} + d_{2}\mathbf{a}_{2}$와 같이 선형결합으로 표현이 가능한 경우, $\mathbf{a}_{1}, \mathbf{a}_{2}, \mathbf{a}_{3}$는 다음과 같이 작성할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    c_{1}\mathbf{a}_{1} + c_{2}\mathbf{a}_{2} + c_{3}\mathbf{a}_{3} = (c_{1}+d_{1})\mathbf{a}_{1} + (c_{1}+d_{1})\mathbf{a}_{2}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org3c662b8" class="outline-2">
<h2 id="org3c662b8"><span class="section-number-2">12</span> Span and Subspace</h2>
<div class="outline-text-2" id="text-12">
<p>
$\mathbb{R}^{n}$공간의 부분공간(Subspace) H는 <b>$\mathbb{R}^{n}$의 부분집합들의 선형결합에 대해 닫혀 있는 공간을 의미한다.</b> 즉, 두 벡터 $\mathbf{u}_{1}, \mathbf{u}_{2} &isin; H$일 때, 어떠한 스칼라 값 $c,d$에 대하여 $c\mathbf{u}_{1} + d \mathbf{u}_{2} &isin; H$일 때 H를 부분공간이라고 한다.
</p>

<p>
Span $\{ \mathbf{a}_{1}, &ctdot;, \mathbf{a}_{n} \}$으로 형성된 공간은 항상 부분공간이다. 만약 $\mathbf{u}_{1} = x_{1}\mathbf{a}_{1} + &ctdot; + x_{n}\mathbf{a}_{n}$이고 $\mathbf{u}_{2} = y_{1}\mathbf{a}_{1} + &ctdot; + y_{n}\mathbf{a}_{n}$일 때
</p>

\begin{equation}
  \begin{aligned}
    c \mathbf{u}_{1} + d \mathbf{u}_{2} &amp; = c(x_{1}\mathbf{a}_{1} + \cdots + x_{n}\mathbf{a}_{n}) + d(y_{1}\mathbf{a}_{1} + \cdots + x_{n}\mathbf{a}_{n}) \\
    &amp; = (cx_{1} + dy_{1})\mathbf{a}_{1} + \cdots + (cx_{n} + dy_{n})\mathbf{a}_{n}
  \end{aligned}
\end{equation}
<p>
과 같이 선형결합으로 나타낼 수 있고 이는 임의의 값 $c,d$에 대해서 닫혀 있음을 의미한다. 따라서 부분공간은 항상 Span $\{ \mathbf{a}_{1}, &ctdot;, \mathbf{a}_{n} \}$으로 표현된다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org5872677" class="outline-2">
<h2 id="org5872677"><span class="section-number-2">13</span> Basis of a Subspace</h2>
<div class="outline-text-2" id="text-13">
<p>
부분공간 H의 <b>기저(basis)</b> 는 다음을 만족하는 벡터들의 집합을 의미한다.
</p>
<ul class="org-ul">
<li>부분공간 H를 모두 Span할 수 있어야 한다.</li>
<li>벡터들 간 선형독립이어야 한다.</li>
</ul>

<p>
3차원 공간 $\mathbb{R}^{3}$의 경우 기저벡터는 3개가 존재하고 $\mathbf{e}_{1} = [ 1 \ 0 \ 0 ]^{T}, \mathbf{e}_{2} = [ 0 \ 1 \  0 ]^{T}, \mathbf{e}_{3} = [ 0 \  0 \  1 ]^{T}$일 때, 이를 <b>표준기저벡터(Standard Basis Vector)라고 한다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-org6537418" class="outline-2">
<h2 id="org6537418"><span class="section-number-2">14</span> Dimension of Subspace</h2>
<div class="outline-text-2" id="text-14">
<p>
하나의 부분공간 H를 표현할 수 있는 기저는 유일하지 않다. 하지만 여러개의 기저를 통해서 표현할 수 있는 부분공간의 차원(Dimension)은 유일하다. <b>부분공간의 차원은 기저벡터의 개수와 동일하다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-org6ddd309" class="outline-2">
<h2 id="org6ddd309"><span class="section-number-2">15</span> Column Space of Matrix</h2>
<div class="outline-text-2" id="text-15">
<p>
행렬 $\mathbf{A}$의 열공간(Column Space)이란 $\mathbf{A}$의 열벡터로 인해 Span된 부분공간을 의미한다. 일반적으로 Col $\mathbf{A}$라고 표기한다.
</p>

\begin{equation}
  \begin{aligned}
    \text{Col} \mathbf{A} = \text{Span} \{ \begin{bmatrix} 1\\1\\0 \end{bmatrix}, \begin{bmatrix} 1 \\0\\1 \end{bmatrix} \}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-orge03b58b" class="outline-2">
<h2 id="orge03b58b"><span class="section-number-2">16</span> Rank of Matrix</h2>
<div class="outline-text-2" id="text-16">
<p>
행렬 $\mathbf{A}$의 rank란 $\mathbf{A}$의 열벡터들의 차원을 의미한다.
</p>

\begin{equation}
  \begin{aligned}
    \text{rank} \mathbf{A} = \text{dim Col} \mathbf{A}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org85695bb" class="outline-2">
<h2 id="org85695bb"><span class="section-number-2">17</span> Transformation</h2>
<div class="outline-text-2" id="text-17">
<p>
변환(Transformation), 함수(Function), 매핑(Mapping) $T$은 입력 $x$를 출력 $y$로 매핑해주는 것을 의미한다.
</p>
\begin{equation}
  \begin{aligned}
    T: x \mapsto y
  \end{aligned}
\end{equation}
<p>
이 때 입력 $x$에 의해 매핑되는 출력 $y$는 유일하게 결정된다. <b>Domain</b> 이란 입력 $x$의 모든 가능한 집합을 의미한다. <b>Co-Domain</b> 이란 출력 $y$의 모든 가능한 집합을 의미한다. <b>Image</b> 란 주어진 입력 $x$에 대해 매핑된 출력 $y$를 의미한다. <b>Range</b> 란 Domain내에 있는 입력 $x$들에 의해 매핑된 모든 출력 $y$의 집합을 의미한다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org54a4274" class="outline-2">
<h2 id="org54a4274"><span class="section-number-2">18</span> Linear Transformation</h2>
<div class="outline-text-2" id="text-18">
<p>
변환 T는 다음과 같은 경우에 선형변환(Linear Transformation)이라고 한다.
</p>

\begin{equation}
  \begin{aligned}
    &amp; T(c \mathbf{u} + d \mathbf{v}) = cT(\mathbf{u}) + vT(\mathbf{v}) \\
    &amp; \text{for all } \mathbf{u,v} \ \ \text{in the domain of T and for all scalars c and d.}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org4ceaaf3" class="outline-2">
<h2 id="org4ceaaf3"><span class="section-number-2">19</span> Transformations between Vectors</h2>
<div class="outline-text-2" id="text-19">
<p>
$T: \mathbf{x}&isin; \mathbb{R}^{n} \mapsto\mathbf{y}&isin; \mathbb{R}^{m}$은 n차원의 벡터를 m차원의 벡터로 매핑하는 연산을 의미한다. 예를 들면 
</p>

\begin{equation}
  \begin{aligned}
    &amp; T: \mathbf{x} \in \mathbb{R}^{3} \mapsto \mathbf{y} \in \mathbb{R}^{2} \\
    &amp; \mathbf{x} = \begin{bmatrix} 1\\2\\3 \end{bmatrix} \in \mathbb{R}^{3} \mapsto \mathbf{y} = T(\mathbf{x}) = \begin{bmatrix} 4\\5 \end{bmatrix} \in \mathbb{R}^{2}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org1e65e30" class="outline-2">
<h2 id="org1e65e30"><span class="section-number-2">20</span> Matrix of Linear Transformation</h2>
<div class="outline-text-2" id="text-20">
<p>
변환 $T: \mathbb{R}^{n} \mapsto\mathbb{R}^{m}$을 선형변환이라고 가정하면 $T$는 항상 행렬과 벡터의 곱으로 표현할 수 있다. 즉,
</p>

\begin{equation}
  \begin{aligned}
    T(\mathbf{x}) = \mathbf{Ax} \ \ \text{for all } \mathbf{x} \in \mathbb{R}^{n}
  \end{aligned}
\end{equation}
<p>
행렬 $\mathbf{A}&isin; \mathbb{R}^{m&times; n}$인 경우 $\mathbf{A}$의 j번째 열 $\mathbf{a}_{j}$는 벡터 $T(\mathbf{e}_{j})$와 같다. 이 때 $\mathbf{e}_{j}$는 항등행렬 $\mathbf{I}&isin; \mathbb{R}^{n&times; n}$의 j번째 열벡터이다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A} = [ T(\mathbf{e}_{1}) \ \cdots \ T(\mathbf{e}_{n})]
  \end{aligned}
\end{equation}
<p>
이러한 행렬 $\mathbf{A}$를 선형변환 T의 표준행렬(Standard Matrix)이라고 부른다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org8eebbea" class="outline-2">
<h2 id="org8eebbea"><span class="section-number-2">21</span> Onto and One-To-One</h2>
<div class="outline-text-2" id="text-21">
<p>
Onto는 <b>전사함수(Surjective)라고도 불리며 공역이 치역과 같은 경우를 의미한다.</b> 이는 Co-Domain의 모든 원소들이 사영된 것을 의미한다.
</p>

\begin{equation}
  \begin{aligned}
    \text{Surjective: Co-Domain = Range}
  \end{aligned}
\end{equation}
<p>
One-To-One은 <b>일대일함수(Injective)라고도 불리며 정의역의 원소와 공역의 원소가 하나씩 대응되는 함수를 의미한다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-org1d91d50" class="outline-2">
<h2 id="org1d91d50"><span class="section-number-2">22</span> Least Square</h2>
<div class="outline-text-2" id="text-22">
<p>
최소제곱법(Least Square)는 <b>방정식의 개수가 미지수의 개수보다 많은 Over-determined 선형시스템에서 사용하는 방법 중 하나이다.</b> Over-determined 선형시스템 $\mathbf{Ax}= \mathbf{b}$의 경우 일반적으로 해가 존재하지 않는다. 이런 경우 일반적으로 $\left\| \mathbf{Ax}- \mathbf{b}\right\|^{2}$가 최소가 되는 근사해를 구할 수 있다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgba00de3" class="outline-2">
<h2 id="orgba00de3"><span class="section-number-2">23</span> Inner Product</h2>
<div class="outline-text-2" id="text-23">
<p>
벡터 $\mathbf{u,v}&isin; \mathbb{R}^{n}$에 대해 이를 각각 $n &times; 1$의 행렬로 생각할 수 있다. 그렇다면 $\mathbf{u}^{T}$는 $1 &times; n$의 행렬로 볼 수 있고 행렬곱 $\mathbf{u}^{T}\mathbf{v}$는 $1 &times;1$의 행렬이 된다. 그리고 $1\times1$행렬은 스칼라값으로 표시할 수 있다.
</p>

<p>
이 때, $\mathbf{u}^{T}\mathbf{v}$에 의해 계산된 값을 $\mathbf{u}, \mathbf{v}$의 내적(Inner Product, Dot Product)라고 한다. 이는 $\mathbf{u}&sdot; \mathbf{v}$로 표기할 수 있다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orga0bc1c6" class="outline-2">
<h2 id="orga0bc1c6"><span class="section-number-2">24</span> Properties of Inner Product</h2>
<div class="outline-text-2" id="text-24">
<p>
벡터 $\mathbf{u,v,w}&isin; \mathbb{R}^{n}$이고 $c$를 스칼라 값이라고 할 때 내적은 다음과 같은 성질을 만족한다.
</p>

<ul class="org-ul">
<li>$\mathbf{u}\cdot \mathbf{v} = \mathbf{v} \cdot \mathbf{u}$</li>
<li>$(\mathbf{u}+\mathbf{v})\cdot \mathbf{w} = \mathbf{u}\cdot \mathbf{w} + \mathbf{v} \cdot \mathbf{w}$</li>
<li>$(c \mathbf{u})\cdot \mathbf{v} = c(\mathbf{u}\cdot \mathbf{v}) = \mathbf{u}\cdot(c \mathbf{v})$</li>
<li>$\mathbf{u}&sdot; \mathbf{u}&ge; 0 $ and $ \mathbf{u}&sdot; \mathbf{u}= 0 $  iff $\mathbf{u}=0$</li>
</ul>

<p>
위에서 2,3번 성질을 조합하면 다음과 같은 법칙을 만들 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    (c_{1}\mathbf{u}_{1} + \cdots + c_{n}\mathbf{u}_{n}) \cdot \mathbf{w} = c_{1}(\mathbf{u}_{1} \cdot \mathbf{w}) + \cdots + c_{n}(\mathbf{u}_{n} \cdot \mathbf{w})
  \end{aligned}
\end{equation}
<p>
위를 통해 <b>내적이라는 연산은 선형변환이라는 것을 알 수 있다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-org7ed0b11" class="outline-2">
<h2 id="org7ed0b11"><span class="section-number-2">25</span> Vector Norm</h2>
<div class="outline-text-2" id="text-25">
<p>
벡터 $\mathbf{v}&isin; \mathbb{R}^{n}$에 대해 벡터의 놈(Norm)은 0이 아닌 $\left\| \mathbf{v}\right\| = \sqrt{\mathbf{v}&sdot; \mathbf{v}}$로 표기하며 벡터의 길이를 의미한다.
</p>

\begin{equation}
  \begin{aligned}
    \left\| \mathbf{v} \right\| = \sqrt{\mathbf{v}\cdot \mathbf{v}} = \sqrt{v_{1}^{2} + v_{2}^{2} + \cdots + v_{n}^{2}}
  \end{aligned}
\end{equation}
<p>
2차원 벡터 $\mathbf{v}&isin; \mathbb{R}^{2}$가 있을 때 $\mathbf{v}= \begin{bmatrix}a \\b\end{bmatrix}$라고 하면 $\left\| v \right\|$는 원점으로부터 $\mathbf{v}$좌표까지의 거리가 된다.
</p>

\begin{equation}
  \begin{aligned}
    \left\| \mathbf{v} \right\| = \sqrt{a^{2} + b^{2}}
  \end{aligned}
\end{equation}
<p>
모든 스칼라 값 $c$에 대해 $c\mathbf{v}$의 길이는 $\mathbf{v}$의 길이를 $| c |$배 한 것을 의미한다.
</p>

\begin{equation}
  \begin{aligned}
    \left\| c \mathbf{v} \right\| = |c| \left\| \mathbf{v} \right\|
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org21a4fae" class="outline-2">
<h2 id="org21a4fae"><span class="section-number-2">26</span> Unit Vector</h2>
<div class="outline-text-2" id="text-26">
<p>
길이가 1인 벡터를 단위벡터(Unit Vector)라고 한다. 벡터의 길이를 1로 맞추는 작업을 정규화(Normalization)라고 하는데 주어진 벡터 $\mathbf{v}$가 있을 때 단위벡터 $\mathbf{u}= \frac{1}{\| \mathbf{v}\|}\mathbf{v}$가 된다. $\mathbf{u}$벡터는 $\mathbf{v}$벡터와 방향은 같지만 크기가 1인 벡터이다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org90d81be" class="outline-2">
<h2 id="org90d81be"><span class="section-number-2">27</span> Distance between Vectors in $\mathbb{R}^{n}$</h2>
<div class="outline-text-2" id="text-27">
<p>
두 벡터 $\mathbf{u,v}&isin; \mathbb{R}^{n}$이 있을 때 두 벡터의 거리는 dist( $\mathbf{u,v}$)로 나타내며 이는 $\mathbf{u-v}$벡터의 길이를 의미한다.
</p>

\begin{equation}
  \begin{aligned}
    dist( \mathbf{u,v}) = \| \mathbf{u} - \mathbf{v} \|
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org02a935a" class="outline-2">
<h2 id="org02a935a"><span class="section-number-2">28</span> Inner Product and Angle between Vectors</h2>
<div class="outline-text-2" id="text-28">
<p>
두 벡터 $\mathbf{u,v}$의 내적은 다음과 같이 놈과 각도를 통해 표현할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{u} \cdot \mathbf{v} = \| \mathbf{u} \| \| \mathbf{v} \| \cos \theta
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org135d4b4" class="outline-2">
<h2 id="org135d4b4"><span class="section-number-2">29</span> Orthogonal Vectors</h2>
<div class="outline-text-2" id="text-29">
<p>
두 벡터 $\mathbf{u,v}&isin; \mathbb{R}^{n}$가 있을 때 둘이 수직이려면 두 벡터의 내적이 0이어야 한다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\| \cos\theta= 0
  \end{aligned}
\end{equation}
<p>
0이 아닌 두 벡터 $\mathbf{u,v}$의 내적이 0이려면 $\cos\theta$값이 0이어야 하고 $\theta=90$degree 일 때 $\cos\theta$값은 0이 된다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgfe1ba64" class="outline-2">
<h2 id="orgfe1ba64"><span class="section-number-2">30</span> Least Square Problem</h2>
<div class="outline-text-2" id="text-30">
<p>
$\mathbf{A}&isin;\mathbb{R}^{m&times; n}, \mathbf{b}&isin;\mathbb{R}^{n}, m &Lt; n$과 같이 주어진 Over-Determined 시스템 $\mathbf{Ax}= \mathbf{b}$가 있을 때 에러의 제곱합 $\|\mathbf{b}-\mathbf{Ax}\|$을 최소화하는 최적의 모델 파라미터를 찾는 것이 목적이 된다. 이 때 최소제곱법의 근사해 $\hat{\mathbf{x}}$는 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{x}} = \arg\min_{\mathbf{x}} \| \mathbf{b}-\mathbf{Ax} \|
  \end{aligned}
\end{equation}
<p>
최소제곱법의 중요한 포인트 중 하나는 어떤 $\mathbf{x}$파라미터를 선정하던지 벡터 $\mathbf{Ax}$는 반드시 Col $\mathbf{A}$안에 위치한다는 것이다. 따라서 <b>최소제곱법은 Col $\mathbf{A}$와 $\mathbf{b}$의 거리가 최소가 되는 $\mathbf{x}$를 찾는 문제가 된다.</b>
</p>

<p>
$\hat{\mathbf{b}} = \mathbf{A}\hat{\mathbf{x}}$를 만족하는 근사해 $\hat{\mathbf{x}}$는 Col $\mathbf{A}$에서 $\mathbf{b}$벡터와 가장 가까운 모든 포인트들의 집합을 의미한다. 따라서 $\mathbf{b}$는 다른 어떤 $\mathbf{Ax}$보다도 $\hat{\mathbf{b}}$와 가장 가깝게 된다. 기하학적으로 이를 만족하기 위해서는 벡터 $\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}$가 Col $\mathbf{A}$와 수직이어야 한다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{b} - \mathbf{A}\hat{\mathbf{x}} \perp (x_{1}\mathbf{a}_{1} + x_{2}\mathbf{a}_{2} + \cdots + x_{n}\mathbf{a}_{n}) \ \ \text{for any vector } \mathbf{x}.
  \end{aligned}
\end{equation}
<p>
이는 곧 다음과 동일하다.
</p>

\begin{equation}
  \begin{aligned}
    &amp; (\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}) \perp \mathbf{a}_{1} \rightarrow \mathbf{a}_{1}^{T}(\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}) \\
    &amp; (\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}) \perp \mathbf{a}_{2} \rightarrow \mathbf{a}_{2}^{T}(\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}) \\
    &amp; (\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}) \perp \mathbf{a}_{3} \rightarrow \mathbf{a}_{3}^{T}(\mathbf{b}-\mathbf{A}\hat{\mathbf{x}}) \\
    &amp; \therefore \mathbf{A}^{T}(\mathbf{b} - \mathbf{A}\hat{\mathbf{x}}) = 0
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org0584794" class="outline-2">
<h2 id="org0584794"><span class="section-number-2">31</span> Normal Equation</h2>
<div class="outline-text-2" id="text-31">
<p>
$\mathbf{Ax}&cong; \mathbf{b}$를 만족하는 최소제곱법의 근사해는 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A}^{T}\mathbf{A} \hat{\mathbf{x}} = \mathbf{A}^{T}\mathbf{b}
  \end{aligned}
\end{equation}
<p>
위 식을 <b>정규방정식(Normal Equation)</b> 이라고 부른다. 이는 $\mathbf{C}= \mathbf{A}^{T}\mathbf{A}&isin; \mathbb{R}^{n&times; n}, \mathbf{d}= \mathbf{A}^{T}\mathbf{b}&isin; \mathbb{R}^{n}$일 때 $\mathbf{Cx}= \mathbf{d}$와 같은 선형시스템으로 생각할 수 있다. 이 선형시스템의 해를 구하면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{x}} = (\mathbf{A}^{T}\mathbf{A})^{-1}\mathbf{A}^{T}\mathbf{b}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org9d2af8b" class="outline-2">
<h2 id="org9d2af8b"><span class="section-number-2">32</span> Another Derivation of Normal Equation</h2>
<div class="outline-text-2" id="text-32">
<p>
근사해 $\hat{\mathbf{x}} = argmin_{\mathbf{x}}\| \mathbf{b}- \mathbf{Ax}\| = argmin_{\mathbf{x}} \| \mathbf{b}- \mathbf{Ax}\|^{2}$와 같이 제곱을 최소화하는 문제로 표현해도 동일한 문제가 된다.
</p>

\begin{equation}
  \begin{aligned}
    \arg\min_{\mathbf{x}}(\mathbf{b}-\mathbf{Ax})^{T}(\mathbf{b}-\mathbf{Ax}) = \mathbf{b}^{T}\mathbf{b} - \mathbf{x}^{T}\mathbf{A}^{T}\mathbf{b} - \mathbf{b}^{T}\mathbf{Ax} + \mathbf{x}^{T}\mathbf{A}^{T}\mathbf{Ax}
  \end{aligned}
\end{equation}
<p>
위 식을 $\mathbf{x}$에 대해서 미분하고 정리하면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    -\mathbf{A}^{T}\mathbf{b} - \mathbf{A}^{T}\mathbf{b} + 2 \mathbf{A}^{T}\mathbf{A}\mathbf{x} = 0 \Leftrightarrow \mathbf{A}^{T}\mathbf{Ax} = \mathbf{A}^{T}\mathbf{b}
  \end{aligned}
\end{equation}
<p>
이 때 $\mathbf{A}^{T}\mathbf{A}$가 역행렬이 존재한다면 다음과 같이 해를 구할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{x} = (\mathbf{A}^{T}\mathbf{A})^{-1}\mathbf{A}^{T}\mathbf{b}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org117f1c8" class="outline-2">
<h2 id="org117f1c8"><span class="section-number-2">33</span> What If $\mathbf{C} =\mathbf{A}^{T}\mathbf{A}$is NOT Invertible?</h2>
<div class="outline-text-2" id="text-33">
<p>
행렬 $\mathbf{C}=\mathbf{A}^{T}\mathbf{A}$의 역행렬이 존재하지 않는 경우 시스템은 해가 없거나 무수히 많은 해를 가지고 있다. 하지만 정규방정식은 항상 해를 가지고 있으므로 해가 없는 상황은 존재하지 않고 실제로는 무수히 많은 해를 가지고 있다. $\mathbf{C}$가 역행렬을 구할 수 없는 경우는 오직 Col $\mathbf{A}$가 선형의존일 경우에 발생한다. 하지만, <b>일반적으로 $\mathbf{C}$는 대부분의 경우 역행렬이 존재한다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-org5f37e66" class="outline-2">
<h2 id="org5f37e66"><span class="section-number-2">34</span> Orthogonal Projection Perspective</h2>
<div class="outline-text-2" id="text-34">
<p>
행렬 $\mathbf{C}=\mathbf{A}^{T}\mathbf{A}$가 있을 때 $\mathbf{b}$점에서 Col $\mathbf{A}$공간으로 프로젝션하면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{b}} = f(\mathbf{b}) = \mathbf{A}\hat{\mathbf{x}} = \mathbf{A}(\mathbf{A}^{T}\mathbf{A})^{-1}\mathbf{A}^{T}\mathbf{b}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org6f46ee6" class="outline-2">
<h2 id="org6f46ee6"><span class="section-number-2">35</span> Orthogonal and Orthonormal Sets</h2>
<div class="outline-text-2" id="text-35">
<p>
벡터들의 집합 $\mathbf{u}_{1},&ctdot;,\mathbf{u}_{n} &isin; \mathbb{R}^{n}$가 있을 때 모든 벡터 쌍들이 $\mathbf{u}_{i} &sdot; \mathbf{u}_{j} = 0, \ \ i &ne; j$를 만족하면 해당 집합은 <b>직교(Orhogonal)</b> 하다고 말한다.
</p>

<p>
벡터들의 집합 $\mathbf{u}_{1},&ctdot;,\mathbf{u}_{n} &isin; \mathbb{R}^{n}$가 있을 때 모든 직교 집합들이 단위벡터인 경우 <b>정규직교(Orthonormal)</b> 하다고 말한다.
</p>

<p>
직교벡터와 정규직교벡터의 집합은 <b>항상 선형독립이다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-org63d1403" class="outline-2">
<h2 id="org63d1403"><span class="section-number-2">36</span> Orthogonal and Orthonormal Basis</h2>
<div class="outline-text-2" id="text-36">
<p>
기저벡터 $\mathbf{u}_{1},&ctdot;,\mathbf{u}_{n}$이 p차원의 부분공간 $W &isin; \mathbb{R}^{n}$에 있다고 할때 Gram-Schmidt 프로세스와 QR decomposition을 사용하면 직교기저벡터를 만들 수 있다. 부분공간 $W$에 대해 직교기저 벡터 $\mathbf{u}_{1},&ctdot;\,\mathbf{u}_{n}$이 주어져 있다고 했을 때 $\mathbf{y}&isin; \mathbb{R}^{n}$을 부분공간 $W$위로 프로젝션시킨다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgdcb9745" class="outline-2">
<h2 id="orgdcb9745"><span class="section-number-2">37</span> Orthogonal Projection $\hat{\mathbf{y}}$of $\mathbf{y}$onto Line</h2>
<div class="outline-text-2" id="text-37">
<p>
1차원 부분공간 $L = \text{Span} \{ \mathbf{u} \}$위로 $\mathbf{y}$를 프로젝션하여 $\hat{\mathbf{y}}$를 구하면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{y}} = \text{proj}_{L} \mathbf{y} = \frac{\mathbf{y}\cdot \mathbf{u}}{\mathbf{u}\cdot \mathbf{u}} \mathbf{u}
  \end{aligned}
\end{equation}
<p>
가 된다. 만약 $\mathbf{u}$가 단위벡터이면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{y}} = \text{proj}_{L} \mathbf{y} = (\mathbf{y}\cdot \mathbf{u}) \mathbf{u}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-orgc5a61e9" class="outline-2">
<h2 id="orgc5a61e9"><span class="section-number-2">38</span> Orthogonal Projection $\hat{\mathbf{y}}$of $\mathbf{y}$onto Plane</h2>
<div class="outline-text-2" id="text-38">
<p>
2차원 부분공간 $W = \text{Span} \{ \mathbf{u}_{1}, \mathbf{u}_{2} \}$위로 $\mathbf{y}$를 프로젝션하여 $\hat{\mathbf{y}}$를 구하면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{y}} =  \text{proj}_{L}\mathbf{y} = \frac{\mathbf{y}\cdot \mathbf{u}_{1}}{\mathbf{u}_{1}\mathbf{u}_{1}}\mathbf{u}_{1} + \frac{\mathbf{y}\cdot \mathbf{u}_{2}}{\mathbf{u}_{2}\mathbf{u}_{2}}\mathbf{u}_{2}
  \end{aligned}
\end{equation}
<p>
만약 $\mathbf{u}_{1}, \mathbf{u}_{2}$가 단위벡터이면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{y}} = \text{proj}_{L} \mathbf{y} = (\mathbf{y} \cdot \mathbf{u}_{1})\mathbf{u}_{1} + (\mathbf{y} \cdot \mathbf{u}_{2}) \mathbf{u}_{2}
  \end{aligned}
\end{equation}
<p>
프로젝션은 각각 직교기저벡터에 독립적으로 적용된다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org434faec" class="outline-2">
<h2 id="org434faec"><span class="section-number-2">39</span> Orthogonal Projection when $\mathbf{y} \in W$</h2>
<div class="outline-text-2" id="text-39">
<p>
만약 2차원 부분공간 $W = \text{Span}\{ \mathbf{u}_{1}, \mathbf{u}_{2} \}$에 $\mathbf{y}$가 포함되어 있다고 하면 프로젝션된 벡터 $\hat{\mathbf{y}}$는 다음과 같이 구할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{y}} =  \text{proj}_{L}\mathbf{y} = \mathbf{y}= \frac{\mathbf{y}\cdot \mathbf{u}_{1}}{\mathbf{u}_{1}\mathbf{u}_{1}}\mathbf{u}_{1} + \frac{\mathbf{y}\cdot \mathbf{u}_{2}}{\mathbf{u}_{2}\mathbf{u}_{2}}\mathbf{u}_{2}
  \end{aligned}
\end{equation}
<p>
만약 $\mathbf{u}_{1}, \mathbf{u}_{2}$가 단위벡터이면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{y}} = \text{proj}_{L} \mathbf{y} = \mathbf{y} = (\mathbf{y} \cdot \mathbf{u}_{1})\mathbf{u}_{1} + (\mathbf{y} \cdot \mathbf{u}_{2}) \mathbf{u}_{2}
  \end{aligned}
\end{equation}
<p>
해는 $\mathbf{y}$가 부분공간 $W$에 포함되어 있지 않은 경우와 동일하다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org5a42fea" class="outline-2">
<h2 id="org5a42fea"><span class="section-number-2">40</span> Transformation: Orthogonal Projection</h2>
<div class="outline-text-2" id="text-40">
<p>
부분공간 $W$의 정규직교기저벡터 $\mathbf{u}_{1}, \mathbf{u}_{2}$가 있고 $\mathbf{b}$를 부분공간 $W$에 프로젝션시킨 점 $\hat{\mathbf{b}}$의 변환을 생각해보면
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{b}} &amp; = f(\mathbf{b}) = (\mathbf{b}\cdot \mathbf{u}_{1})\mathbf{u}_{1} + (\mathbf{b}\cdot \mathbf{u}_{2})\mathbf{u}_{2} \\
    &amp; = (\mathbf{u}_{1}^{T} \mathbf{b})\mathbf{u}_{1} + (\mathbf{u}_{2}^{T} \mathbf{b})\mathbf{u}_{2} \\
    &amp; = \mathbf{u}_{1}(\mathbf{u}_{1}^{T}\mathbf{b}) + \mathbf{u}_{2}(\mathbf{u}_{2}^{T}\mathbf{b}) \\
    &amp; = (\mathbf{u}_{1}\mathbf{u}_{1}^{T})\mathbf{b} + (\mathbf{u}_{2}\mathbf{u}_{2}^{T})\mathbf{b} \\
    &amp; = (\mathbf{u}_{1}\mathbf{u}_{1}^{T} + \mathbf{u}_{2}\mathbf{u}_{2}^{T}) \mathbf{b} \\
    &amp; = [ \mathbf{u}_{1} \ \mathbf{u}_{2} ] \begin{bmatrix} \mathbf{u}_{1}^{T} \\ \mathbf{u}_{2}^{T} \end{bmatrix}\mathbf{b} = \mathbf{UU}^{T} \mathbf{b} \Rightarrow \ \text{Linear Transformation!}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-orga0ff7ea" class="outline-2">
<h2 id="orga0ff7ea"><span class="section-number-2">41</span> Orthogonal Projection Perspective</h2>
<div class="outline-text-2" id="text-41">
<p>
정규직교인 열벡터를 가지는 행렬 $\mathbf{A}= \mathbf{U}=  [\mathbf{u}_{1} \ \mathbf{u}_{2}]$가 있을 때 $\mathbf{b}$벡터를 Col $\mathbf{A}$공간으로 정사영시키는 경우
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{b}} = \mathbf{A}\hat{\mathbf{x}} = \mathbf{A}(\mathbf{A}^{T}\mathbf{A})^{-1}\mathbf{A}^{T}\mathbf{b} = f(\mathbf{b})
  \end{aligned}
\end{equation}
<p>
행렬 $\mathbf{C}= \mathbf{A}^{T}\mathbf{A}$는 $\mathbf{C}= \begin{bmatrix}\mathbf{u}_{1}^{T} \\ \mathbf{u}_{2}^{T} \end{bmatrix}\begin{bmatrix}\mathbf{u}_{1} \ \mathbf{u}_{2} \end{bmatrix}= \mathbf{I}$와 같은 성질을 지니게 되고 따라서 다음과 같은 공식이 성립한다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{b}} = \mathbf{A}\hat{\mathbf{x}} = \mathbf{A}(\mathbf{A}^{T}\mathbf{A})^{-1}\mathbf{A}^{T}\mathbf{b} = \mathbf{A}(\mathbf{I})^{-1}\mathbf{A}^{T}\mathbf{b} = \mathbf{AA}^{T}\mathbf{b} = \mathbf{UU}^{T}\mathbf{b}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org8d04877" class="outline-2">
<h2 id="org8d04877"><span class="section-number-2">42</span> Gram-Schmidt Orthogonalization</h2>
<div class="outline-text-2" id="text-42">
<p>
벡터 $\mathbf{x}_{1} = \begin{bmatrix}3\\6\\0 \end{bmatrix}, \mathbf{x}_{2} = \begin{bmatrix}1\\2\\2 \end{bmatrix}$로 인해 Span되는 부분공간 $W{x}_{1} = \text{Span}[ \mathbf{x}\ \mathbf{x}_{2} ]$가 있을 때 두 벡터의 내적 $\mathbf{x}_{1}&sdot; \mathbf{x}_{2} = 15 &ne; 0$이므로 두 벡터는 수직이 아니다.
</p>

<p>
이 때 벡터 $\mathbf{v}_{1} = \mathbf{x}_{1}$이라고 하고 $\mathbf{v}_{2}$를 $\mathbf{x}_{1}$에 수직인 $\mathbf{x}_{2}$의 성분이라고 했을 때
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{v}_{2} = \mathbf{x}_{2} - \frac{\mathbf{x}_{2}\cdot \mathbf{x}_{1}}{\mathbf{x}_{1}\cdot \mathbf{x}_{1}} \mathbf{x}_{1} = \begin{bmatrix} 1\\2\\2 \end{bmatrix} - \frac{15}{45} \begin{bmatrix} 3\\6\\0 \end{bmatrix} = \begin{bmatrix} 0\\0\\2 \end{bmatrix}
  \end{aligned}
\end{equation}
<p>
가 된다. 이 때 벡터 $\mathbf{v}_{1}, \mathbf{v}_{2}$는 부분공간 $W$의 직교기저벡터가 된다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgf499216" class="outline-2">
<h2 id="orgf499216"><span class="section-number-2">43</span> Eigenvectors and Eigenvalues</h2>
<div class="outline-text-2" id="text-43">
<p>
정방행렬 $\mathbf{A}&isin; \mathbb{R}^{n&times; n}$에 대한 고유벡터(eigenvector)는 $\mathbf{Ax}=&lambda; \mathbf{x}$를 만족하는 0이 아닌 벡터 $\mathbf{x}&isin; \mathbb{R}^{n}$을 말한다. 이 때 $&lambda;$는 행렬 $\mathbf{A}$의 고유값(eigenvalue)이라고 한다.
</p>

<p>
$\mathbf{Ax}= &lambda; \mathbf{x}$는 다음과 같이 다시 나타낼 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    (\mathbf{A} - \lambda \mathbf{I})\mathbf{x} = 0
  \end{aligned}
\end{equation}
<p>
이 때, 위 시스템이 $\mathbf{x}$가 0이 아닌 비자명해를 가지고 있는 경우에만 $\lambda$값이 행렬 $\mathbf{A}$에 대한 고유값이 된다. 위와 같은 동차 선형시스템이 비자명해를 가지기 위해서는 $\mathbf{A}-&lambda; \mathbf{I}$가 선형의존(Linearly Dependent)해야 무수히 많은 해를 가진다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgefcb36d" class="outline-2">
<h2 id="orgefcb36d"><span class="section-number-2">44</span> Null Space</h2>
<div class="outline-text-2" id="text-44">
<p>
행렬 $\mathbf{A}&isin; \mathbb{R}^{m &times; n}$의 동차 선형시스템(Homogeneous Linear System) $\mathbf{Ax}= 0$의 해 집합을 영공간(Null Space)라고 한다. Nul $\mathbf{A}$로 표기한다.
</p>

<p>
$\mathbf{A}= \begin{bmatrix}\mathbf{a}_{1}^{T} \\ \mathbf{a}_{2}^{T} \\ \vdots\\ \mathbf{a}_{m}^{T} \end{bmatrix}$일 때 벡터 $\mathbf{x}$는 다음을 만족해야 한다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{a}_{1}^{T}\mathbf{x} = \mathbf{a}_{2}\mathbf{x} = \cdots = \mathbf{a}_{m}^{T}\mathbf{x} = 0
  \end{aligned}
\end{equation}
<p>
즉, $\mathbf{x}$는 모든 $\mathbf{A}$의 행벡터(Row Vector)과 직교해야 한다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org82161f1" class="outline-2">
<h2 id="org82161f1"><span class="section-number-2">45</span> Orthogonal Complement</h2>
<div class="outline-text-2" id="text-45">
<p>
벡터 $\mathbf{z}$가 부분공간 $W &isin; \mathbb{R}^{n}$의 모든 벡터와 직교하면 $\mathbf{z}$는 부분공간 $W$와 직교한다고 말할 수 있다. 부분공간 $W$와 직교하는 모든 벡터 $\mathbf{z}$의 집합을 직교여공간(Orthogonal Complement)라고 부르며 $W^{&perp;}$로 표시한다.
</p>

<p>
부분공간 $W$의 직교여공간 $W^{&perp;}$에 위치한 벡터 $\mathbf{x}&isin; \mathbb{R}^{n}$는 부분공간 $W$를 Span하는 모든 벡터들과 직교한다.
</p>

\begin{equation}
  \begin{aligned}
    &amp; W^{\perp} \ \ \text{is a subspace of } \ \mathbb{R}^{n}. \\
    &amp; \text{Nul} \mathbf{A} = (\text{Row} \mathbf{A})^{\perp} \\
    &amp; \text{Nul} \mathbf{A}^{T} = (\text{Col} \mathbf{A})^{\perp} \\
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org3d74862" class="outline-2">
<h2 id="org3d74862"><span class="section-number-2">46</span> Characteristic Equation</h2>
<div class="outline-text-2" id="text-46">
<p>
방정식 $(\mathbf{A}-&lambda; \mathbf{I})\mathbf{x}= 0$이 비자명해를 갖기 위해서는 $(\mathbf{A}-\lambda \mathbf{I})$행렬이 선형의존이어야 하고 이는 곧 역행렬이 존재하지 않아야 하는 것과 동치(Equivalent)이다. 만약 $(\mathbf{A}-&lambda; \mathbf{I})$이 역행렬이 존재한다면 $\mathbf{x}$는 자명해 이외에는 갖지 못한다.
</p>

\begin{equation}
  \begin{aligned}
    &amp; (\mathbf{A}-\lambda \mathbf{I})^{-1}(\mathbf{A}-\lambda \mathbf{I})\mathbf{x} = (\mathbf{A}-\lambda \mathbf{I})^{-1} 0 \\
    &amp; \mathbf{x} = 0
  \end{aligned}
\end{equation}
<p>
따라서 행렬 $\mathbf{A}$에 대하여 고유값과 고유벡터가 존재하기 위해서는 다음의 방정식이 항상 성립해야 한다.
</p>

\begin{equation}
  \begin{aligned}
    \det(\mathbf{A}-\lambda \mathbf{I}) = 0
  \end{aligned}
\end{equation}
<p>
위 방정식을 행렬 $\mathbf{A}$의 특성방정식(Characteristic Equation)이라고 부른다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orge17df62" class="outline-2">
<h2 id="orge17df62"><span class="section-number-2">47</span> Eigenspace</h2>
<div class="outline-text-2" id="text-47">
<p>
$(\mathbf{A}-&lambda; \mathbf{x})\mathbf{x}=0$에서 $(\mathbf{A}-&lambda; \mathbf{x})$의 영공간(Null Space)를 고유값 $&lambda;$에 대한 고유공간(Eigenspace)라고 한다. $&lambda;$에 대한 고유공간의 차원이 1 이상인 경우, 고유공간 내에 있는 모든 벡터들에 대하여 다음이 성립한다.
</p>

\begin{equation}
  \begin{aligned}
    T(\mathbf{x}) = \mathbf{Ax} = \lambda \mathbf{x}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org0237cdc" class="outline-2">
<h2 id="org0237cdc"><span class="section-number-2">48</span> Diagonalization</h2>
<div class="outline-text-2" id="text-48">
<p>
정방행렬 $\mathbf{A}&isin; \mathbb{R}^{n&times; n}$이 주어졌고 $\mathbf{V}&isin; \mathbb{R}^{n&times; n}$이고 $\mathbf{D}&isin; \mathbb{R}^{n&times; n}$일 때
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{D} =  \mathbf{V}^{-1}\mathbf{AV}
  \end{aligned}
\end{equation}
<p>
위와 같은 공식이 성립한다면 이를 <b>정방행렬 $\mathbf{A}$의 대각화(Diagonalization)라고 한다.</b> 대각화는 모든 경우에 대해서 항상 가능한 것은 아니다. 행렬 $\mathbf{A}$가 대각화되기 위해서는 역행렬이 존재하는 행렬 $\mathbf{V}$가 존재해야 한다. 행렬 $\mathbf{V}$가 역행렬이 존재하기 위해서는 $\mathbf{V}$는 <b>행렬 $\mathbf{A}$와 같은 $\mathbb{R}^{n \times n}$크기의 정방행렬이어야 하고 n개의 선형독립인 열벡터를 가지고 있어야 한다.</b> 이 때, $\mathbf{V}$의 각 열은 행렬 $\mathbf{A}$의 고유벡터가 된다. 만약 행렬 $\mathbf{V}$가 존재하는 경우 행렬 $\mathbf{A}$는 <b>대각화 가능(Diangonalizable)하다</b> 고 한다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org2965500" class="outline-2">
<h2 id="org2965500"><span class="section-number-2">49</span> Finding $\mathbf{V}$and $\mathbf{D}$</h2>
<div class="outline-text-2" id="text-49">
<p>
대각화 공식은 다음과 같이 다시 작성할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{D} = \mathbf{V}^{-1}\mathbf{AV} \Rightarrow \mathbf{VD} = \mathbf{AV}
  \end{aligned}
\end{equation}
<p>
이 때, $\mathbf{V}= \begin{bmatrix}\mathbf{v}_{1} &amp; \mathbf{v}_{2} &amp; &ctdot; &amp; \mathbf{v}_{n} \end{bmatrix}$이고 $\mathbf{D}= \begin{bmatrix}&lambda;_{1} &amp; 0 &amp; &ctdot; &amp; 0 \\ 0 &amp; &lambda;_{2} &amp; \ddots&amp; \vdots\\ \vdots&amp; \ddots&amp; \ddots&amp; 0 \\ 0 &amp; &ctdot; &amp; 0 &amp; &lambda;_{n} \end{bmatrix}$이라고 하면
</p>

\begin{equation}
  \begin{aligned}
    &amp; \mathbf{AV}  = \mathbf{A} \begin{bmatrix} \mathbf{v}_{1} &amp; \mathbf{v}_{2} &amp; \cdots &amp; \mathbf{v}_{n} \end{bmatrix} = \begin{bmatrix} \mathbf{Av}_{1} &amp; \mathbf{Av}_{2} &amp; \cdots &amp; \mathbf{Av}_{n} \end{bmatrix} \\
    &amp; \mathbf{VD}  = \begin{bmatrix} \mathbf{v}_{1} &amp; \mathbf{v}_{2} &amp; \cdots &amp; \mathbf{v}_{n} \end{bmatrix} \begin{bmatrix} \lambda_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_{2} &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 0 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda_{n} \end{bmatrix} \\
    &amp; \quad \quad = \begin{bmatrix} \lambda_{1}\mathbf{v}_{1} &amp; \lambda_{2}\mathbf{v}_{2} &amp; \cdots &amp; \lambda_{n}\mathbf{v}_{n} \end{bmatrix} \\
    &amp; \mathbf{AV} = \mathbf{VD} \Leftrightarrow \begin{bmatrix} \mathbf{Av}_{1} &amp; \mathbf{Av}_{2} &amp; \cdots &amp; \mathbf{Av}_{n} \end{bmatrix} = \begin{bmatrix} \lambda_{1}\mathbf{v}_{1} &amp; \lambda_{2}\mathbf{v}_{2} &amp; \cdots &amp; \lambda_{n}\mathbf{v}_{n} \end{bmatrix}
  \end{aligned}
\end{equation}
<p>
위 공식과 같이
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{Av}_{1} = \lambda_{1}\mathbf{v}_{1}, \mathbf{Av}_{2} = \lambda_{2}\mathbf{v}_{2}, \cdots, \mathbf{Av}_{n} = \lambda_{n}\mathbf{v}_{n}
  \end{aligned}
\end{equation}
<p>
각각의 열이 모두 동일해야 한다. 즉, <b>벡터 $\mathbf{v}_{i}$는 행렬 $\mathbf{A}$에 대한 고유벡터가 되어야 하고 스칼라 $&lambda;_{i}$는 행렬 $\mathbf{A}$에 대한 고유값이 되어야 한다.</b> 이에 따라 대각행렬 $\mathbf{D}$는 고유값들을 대각성분으로 포함하고 있는 행렬이 된다. 결론적으로 <b>정방행렬 $\mathbf{A}&isin; \mathbb{R}^{n&times; n}$가 대각화 가능한가 안한가에 대한 질문은 n개의 고유벡터가 존재하는가 안하는가에 대한 질문과 동치이다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-org8f1174e" class="outline-2">
<h2 id="org8f1174e"><span class="section-number-2">50</span> Eigendecomposition</h2>
<div class="outline-text-2" id="text-50">
<p>
정방행렬 $\mathbf{A}$가 대각화 가능한 경우 $\mathbf{D} = \mathbf{V}^{-1}\mathbf{AV}$공식이 성립한다. 이 공식을 다시 작성하면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A} = \mathbf{VDV}^{-1}
  \end{aligned}
\end{equation}
<p>
이를 행렬 $\mathbf{A}$에 대한 <b>고유값 분해(Eigendecomposition)</b> 라고 한다. 행렬 $\mathbf{A}$가 대각화 가능하다는 의미는 행렬 $\mathbf{A}$가 고유값 분해 가능하다는 말과 동치이다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org34c3cd6" class="outline-2">
<h2 id="org34c3cd6"><span class="section-number-2">51</span> Linear Transformation via Eigendecomposition</h2>
<div class="outline-text-2" id="text-51">
<p>
정방행렬 $\mathbf{A}$가 대각화 가능한 경우 $\mathbf{A}= \mathbf{VDV}^{-1}$과 같이 고유값 분해가 가능하다. 이 때 선형 변환 $T(\mathbf{x}) = \mathbf{Ax}$을 생각해보면 다음과 같이 표현할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    T(\mathbf{x}) = \mathbf{Ax} = \mathbf{VDV}^{-1}\mathbf{x} = \mathbf{V}(\mathbf{D}(\mathbf{V}^{-1}\mathbf{x}))
  \end{aligned}
\end{equation}
</div>
<div id="outline-container-org5a68d84" class="outline-3">
<h3 id="org5a68d84"><span class="section-number-3">51.1</span> Change of Basis</h3>
<div class="outline-text-3" id="text-51-1">
<p>
예를 들어 $\mathbf{Av}_{1} = -1 \mathbf{v}_{1}, \mathbf{Av}_{2} = 2 \mathbf{v}_{2}$가 성립한다고 가정하고 $T(\mathbf{x}) = \mathbf{Ax}= \mathbf{VDV}^{-1}\mathbf{x}= \mathbf{V}(\mathbf{D}(\mathbf{V}^{-1}\mathbf{x}))$에서 $\mathbf{y}= \mathbf{V}^{-1}\mathbf{x}$라고 가정하면
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{Vy} = \mathbf{x}
  \end{aligned}
\end{equation}
<p>
의 관계가 성립한다. 이 때, <b>벡터 $\mathbf{y}$는 벡터 $\mathbf{x}$의 고유벡터 $\{ \mathbf{v}_{1}, \mathbf{v}_{2} \}$에 대한 새로운 좌표를 의미한다.</b>
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{x} = \begin{bmatrix} 4\\3 \end{bmatrix} = 4 \begin{bmatrix} 1\\0 \end{bmatrix} + 3 \begin{bmatrix} 0 \\1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} 4 \\ 3 \end{bmatrix} = \mathbf{Vy} = [ \mathbf{v}_{1} \ \ \mathbf{v}_{2} ] \begin{bmatrix} y_{1} \\y_{2} \end{bmatrix} = 2 \mathbf{v}_{1} + 1 \mathbf{v}_{2} \Rightarrow \mathbf{y} = \begin{bmatrix} 2\\1 \end{bmatrix}
  \end{aligned}
\end{equation}
</div>
</div>
<div id="outline-container-org6a79021" class="outline-3">
<h3 id="org6a79021"><span class="section-number-3">51.2</span> Element-wise Scaling</h3>
<div class="outline-text-3" id="text-51-2">
<p>
위 과정을 통해 $\mathbf{y}$값을 구하고 나면 $T(\mathbf{x}) = \mathbf{V}(\mathbf{D}(\mathbf{V}^{-1}\mathbf{x}))$는 $T(\mathbf{x}) = \mathbf{V}(\mathbf{Dy})$로 표현할 수 있다. 이 때 $\mathbf{z}= \mathbf{Dy}$라고 하면 벡터 $\mathbf{z}$는 단순히 벡터 $\mathbf{y}$를 행렬의 대각 원소의 크기만큼 스케일링한 벡터가 된다.
</p>
</div>
</div>
<div id="outline-container-orgd6fece8" class="outline-3">
<h3 id="orgd6fece8"><span class="section-number-3">51.3</span> Back to Original Basis</h3>
<div class="outline-text-3" id="text-51-3">
<p>
위 과정까지 진행했으면 $T(\mathbf{x}) = \mathbf{V}(\mathbf{Dy}) = \mathbf{Vz}$와 같이 나타낼 수 있고 이 때 벡터 $\mathbf{z}$는 여전히 새로운 기저벡터 $\{ \mathbf{v}_{1}^{'} \ \ \mathbf{v}_{2}^{'} \}$를 기반으로 하면 좌표가 된다. <b>$\mathbf{Vz}$연산은 벡터 $\mathbf{z}$를 다시 원래 기저벡터의 좌표로 변환하는 역할을 한다.</b> 벡터 $\mathbf{Vz}$는 기존의 기저벡터 $\{ \mathbf{v}_{1} \ \ \mathbf{v}_{2} \}$의 선형결합이 된다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{Vz} = \begin{bmatrix} \mathbf{v}_{1} &amp; \mathbf{v}_{2} \end{bmatrix} \begin{bmatrix} z_{1} \\ z_{2} \end{bmatrix} = \mathbf{v}_{1}z_{1} + \mathbf{v}_{2}z_{2}
  \end{aligned}
\end{equation}
<p>
지금까지의 과정을 <b>고유값 분해를 통한 선형 변환</b> 이라고 한다.
</p>

<hr />
</div>
</div>
</div>
<div id="outline-container-org18cafa3" class="outline-2">
<h2 id="org18cafa3"><span class="section-number-2">52</span> Linear Transformation via $\mathbf{A}^{k}$</h2>
<div class="outline-text-2" id="text-52">
<p>
여러번의 변환이 중첩된 $\mathbf{A}&times; \mathbf{A}&times; &ctdot; &times; \mathbf{Ax}= \mathbf{A}^{k}\mathbf{x}$를 생각해보자. 이 때, 행렬 $\mathbf{A}$가 대각화 가능하다면 $\mathbf{A}$를 고유값 분해할 수 있고 이 때, $\mathbf{A}^{k}$는 다음과 같이 분해할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A}^{k} = (\mathbf{VDV}^{-1}) (\mathbf{VDV}^{-1}) \cdots (\mathbf{VDV}^{-1}) = \mathbf{VD}^{k}\mathbf{V}^{-1}
  \end{aligned}
\end{equation}
<p>
이 때 $\mathbf{D}^{k}$는 다음과 같이 표현된다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{D}^{k} = \begin{bmatrix} \lambda_{1}^{k} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_{2}^{k} &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 0 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda_{n}^{k} \end{bmatrix}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org75ccd94" class="outline-2">
<h2 id="org75ccd94"><span class="section-number-2">53</span> Geometric Multiplicity and Algebraic Multiplicity</h2>
<div class="outline-text-2" id="text-53">
<p>
정방행렬 $\mathbf{A}&isin; \mathbb{R}^{n&times; n}$이 있을 때 $\mathbf{A}$가 대각화 가능한지 안한지 판단을 해야하는 경우 일반적으로 판별식을 사용하여 판단한다.
</p>

\begin{equation}
  \begin{aligned}
    \det (\mathbf{A} - \lambda \mathbf{I})= 0
  \end{aligned}
\end{equation}
<p>
예를 들어 $n=5$인 정방행렬 $\mathbf{A}$가 있을 때, $det (\mathbf{A}- &lambda; \mathbf{I})$는 5차 다항식이 나오게 된다. 5차 다항식은 일반적으로 5개의 해를 가지고 있지만 실수만 고려하는 경우 5개의 해가 계산되지 않을 수 있다. 즉, <b>실근이 5개가 나오지 않는 경우 $n=5$개의 선형독립인 고유벡터가 나오지 않으므로 대각화가 불가능하다.</b>
</p>

<p>
만약 실근 중 중근이 포함되는 경우, 예를 들어 $(&lambda;-2)^{2}(&lambda; -3) = 0$과 같이 $&lambda; = 2$가 중근인 경우, <b>$&lambda;=2$로 인해 생성되는 고유공간(Eigenspace)의 차원이 최대 $&lambda;=2$가 가지는 중근의 개수까지 가질 수 있다.</b> 중근이 아닌 일반 실근의 경우 최대 1차원의 고유공간을 가질 수 있다. <b>즉, 중근이 포함된 경우 고유공간의 차원이 최대 $n=5$까지 생성될 수 있는데 $n=5$를 만족하지 못하는 경우에는 대각화가 불가능하다.</b>
</p>

<p>
이와 같이 대수적으로 판별식을 인수분해했을 때, 중근이 생기는 경우 중근의 <b>대수 중복도(Algebraic Multiplicity)</b> 와 이로 인해 Span되는 고유공간의 <b>기하 중복도(Geometric Multiplicity)</b> 가 일치해야 $n$개의 독립적인 고유벡터가 생성될 수 있고 행렬 $\mathbf{A}$의 대각화가 가능하다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org8f11c7e" class="outline-2">
<h2 id="org8f11c7e"><span class="section-number-2">54</span> Singular Value Decomposition</h2>
<div class="outline-text-2" id="text-54">
<p>
행렬 $\mathbf{A}&isin; \mathbb{R}^{m &times; n}$이 주어졌을 때 특이값 분해(Singular Value Decomposition, SVD)는 다음과 같이 나타낼 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A} = \mathbf{U} \Sigma \mathbf{V}^{T}
  \end{aligned}
\end{equation}
<p>
이 때, $\mathbf{U}&isin; \mathbb{R}^{m &times; m}, \mathbf{V}&isin; \mathbb{R}^{n&times; n}$인 행렬이며 이들은 각 열이 Col $\mathbf{A}$와 Row $\mathbf{A}$에 의 정규직교기저벡터(Orthonormal Basis)로 구성되어 있다. $&Sigma; &isin; \mathbb{R}^{m&times; n}$은 대각행렬이며 대각 성분들이 $\sigma_{1} \ge \sigma_{2} \ge \cdots \ge \sigma_{\min(m,n)}$특이값이며 큰 값부터 내림차순으로 정렬된 행렬이다. 
</p>

<hr />
</div>
</div>
<div id="outline-container-org139aebd" class="outline-2">
<h2 id="org139aebd"><span class="section-number-2">55</span> SVD as Sum of Outer Products</h2>
<div class="outline-text-2" id="text-55">
<p>
행렬 $\mathbf{A}$는 다음과 같이 Outer Products의 합으로 표현할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A} = \mathbf{U}\Sigma \mathbf{V}^{T} = \sum^{n}_{i=1} \sigma_{i} \mathbf{u}_{i} \mathbf{v}_{i}^{T}, \quad \text{where } \ \sigma_{1}  \ge \sigma_{2} \ge \cdots \ge \sigma_{n}
  \end{aligned}
\end{equation}
<p>
이 때 위 식을 다시 행렬로 합성하면 $\mathbf{U} \in \mathbb{R}^{m\times m} \rightarrow \mathbf{U}^{'} \in \mathbb{R}^{m\times n}$그리고 $\mathbf{D}&isin; \mathbb{R}^{m&times; n} &rarr; \mathbf{D}^{'} &isin; \mathbb{R}^{n&times; n}$과 같이 행렬 $\mathbf{V}^{T}$의 차원에 맞게 다시 합성할 수 있는데 이를 <b>Reduced Form of SVD</b> 이라고 한다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{A} = \mathbf{U}^{'}\mathbf{D}^{'}\mathbf{V}^{T}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org8d34ee7" class="outline-2">
<h2 id="org8d34ee7"><span class="section-number-2">56</span> Another Perspective of SVD</h2>
<div class="outline-text-2" id="text-56">
<p>
행렬 $\mathbf{A}&isin; \mathbb{R}^{m&times; n}$에 대해 Gram-Schmidt Orthogonalization을 사용하면 Col $\mathbf{A}$에 대한 정규직교기저벡터 $\mathbf{u}_{1},&ctdot;,\mathbf{u}_{n}$와 Row $\mathbf{A}$에 대한 정규직교기저벡터 $\mathbf{v}_{1},&ctdot;,\mathbf{v}_{n}$을 구할 수 있다. 하지만 이렇게 계산한 정규직교기저벡터 $\mathbf{u}_{i}, \mathbf{v}_{i}$는 유일하지 않다.
</p>

<p>
Reduced Form of SVD를 사용하면 행렬 $\mathbf{U}= \begin{bmatrix}\mathbf{u}_{1} &amp; \mathbf{u}_{2} &amp; &ctdot; &amp; \mathbf{u}_{n} \end{bmatrix}&isin; \mathbb{R}^{m&times; n}$과 $\mathbf{V} = \begin{bmatrix} \mathbf{v}_{1} &amp; \mathbf{v}_{2} &amp; \cdots &amp; \mathbf{v}_{n} \end{bmatrix} \in \mathbb{R}^{n}$그리고 $\Sigma = \begin{bmatrix} \sigma_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma_{2} &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 0 \\ 0 &amp; \cdots &amp; 0 &amp; \sigma_{n} \end{bmatrix} \in \mathbb{R}^{n\times n}$일 때
</p>

\begin{equation}
  \begin{aligned}
    &amp; \mathbf{AV} = \mathbf{A} \begin{bmatrix} \mathbf{v}_{1} &amp; \mathbf{v}_{2} &amp; \cdots &amp; \mathbf{v}_{n} \end{bmatrix} = \begin{bmatrix} \mathbf{Av}_{1} &amp; \mathbf{Av}_{2} &amp; \cdots &amp; \mathbf{Av}_{n} \end{bmatrix} \\
    &amp; \mathbf{U}\Sigma = \begin{bmatrix} \mathbf{u}_{1} &amp; \mathbf{u}_{2} &amp; \cdots &amp; \mathbf{u}_{n} \end{bmatrix}  \begin{bmatrix} \sigma_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma_{2} &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 0 \\ 0 &amp; \cdots &amp; 0 &amp; \sigma_{n} \end{bmatrix} \\
    &amp; \quad \quad \begin{bmatrix} \sigma_{1} \mathbf{u}_{1} &amp; \sigma_{2} \mathbf{u}_{2} &amp; \cdots &amp; \sigma_{n}\mathbf{u}_{n} \end{bmatrix} \\
    &amp; \mathbf{AV} = \mathbf{U}\Sigma \Leftrightarrow \begin{bmatrix} \mathbf{Av}_{1} &amp; \mathbf{Av}_{2} &amp; \cdots &amp; \mathbf{Av}_{n} \end{bmatrix} = \begin{bmatrix} \sigma_{1} \mathbf{u}_{1} &amp; \sigma_{2} \mathbf{u}_{2} &amp; \cdots &amp; \sigma_{n}\mathbf{u}_{n} \end{bmatrix}
  \end{aligned}
\end{equation}
<p>
위 식을 간결하게 나타내면 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{AV} = \mathbf{U}\Sigma \Leftrightarrow \mathbf{A} = \mathbf{U}\Sigma \mathbf{V}^{T}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org1030ef5" class="outline-2">
<h2 id="org1030ef5"><span class="section-number-2">57</span> Computing SVD</h2>
<div class="outline-text-2" id="text-57">
<p>
행렬 $\mathbf{A}&isin; \mathbb{R}^{m&times; n}$에 대하여 $\mathbf{AA}^{T}$와 $\mathbf{A}^{T}\mathbf{A}$는 다음과 같이 고유값분해할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    &amp; \mathbf{AA}^{T} = \mathbf{U}\Sigma \mathbf{V}^{T} \mathbf{V} \Sigma^{T} \mathbf{U}^{T} = \mathbf{U}\Sigma\Sigma^{T}\mathbf{U}^{T} = \mathbf{U}\Sigma^{2}\mathbf{U}^{T} \\
    &amp; \mathbf{A}^{T}\mathbf{A} = \mathbf{V}\Sigma^{T}\mathbf{U}^{T} \mathbf{U}\Sigma \mathbf{V}^{T} = \mathbf{V}\Sigma^{T}\Sigma \mathbf{U}^{T} = \mathbf{V}\Sigma^{2}\mathbf{V}^{T}
  \end{aligned}
\end{equation}
<p>
이 때 계산되는 행렬 $\mathbf{U}, \mathbf{V}$은 직교하는 고유벡터를 각 열의 성분으로 하는 행렬이며 대각행렬 $&Sigma;^{2}$의 각 성분은 항상 0보다 큰 양수의 값을 가진다. 그리고 $\mathbf{AA}^{T}$와 $\mathbf{A}^{T}\mathbf{A}$를 통해 계산되는 $&Sigma;^{2}$의 값은 동일하다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgcc5efd7" class="outline-2">
<h2 id="orgcc5efd7"><span class="section-number-2">58</span> Diagonalization of Symmetric Matrices</h2>
<div class="outline-text-2" id="text-58">
<p>
일반적으로 정방행렬 $\mathbf{A}&isin; \mathbb{R}^{n&times; n}$이 $n$개의 선형독립인 고유벡터를 가지고 있을 경우 대각화 가능하다. 그리고 대칭행렬 <b>$\mathbf{S}&isin;\mathbb{R}^{n&times; n}, \mathbf{S}^{T} = \mathbf{S}$는 항상 대각화 가능하다.</b> 추가적으로 <b>대칭행렬 $\mathbf{S}$의 고유벡터는 항상 서로에게 직교하므로 직교대각화(Orthogonally Diagonalizable)가 가능하다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-orgc2fa864" class="outline-2">
<h2 id="orgc2fa864"><span class="section-number-2">59</span> Spectral Theorem of Symmetric Matrices</h2>
<div class="outline-text-2" id="text-59">
<p>
$\mathbf{S}^{T} = \mathbf{S}$를 만족하는 대칭행렬 $\mathbf{S}$가 주어졌을 때 $\mathbf{S}$는 $n$개의 중근을 포함한 실수의 고유값이 존재한다. 또한, 고유공간의 차원은 기하 중복도(Algebraic Multiplicity)와 기하 중복도(Geometric Multiplicity)와 같아야 한다. 서로 다른 $&lambda;$값 들에 대한 고유공간들은 서로 직교한다. 결론적으로 대칭행렬 $\mathbf{S}$은 직교대각화가 가능하다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgee73dfb" class="outline-2">
<h2 id="orgee73dfb"><span class="section-number-2">60</span> Spectral Decomposition</h2>
<div class="outline-text-2" id="text-60">
<p>
대칭행렬 $\mathbf{S}$의 고유값 분해는 <b>Spectral Decomposition</b> 이라고 불린다. 이는 다음과 같이 나타낼 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{S} &amp; = \mathbf{UDU}^{-1} = \mathbf{UDU}^{T} = \begin{bmatrix} \mathbf{u}_{1} &amp; \mathbf{u}_{2} &amp; \cdots &amp; \mathbf{u}_{n} \end{bmatrix}  \begin{bmatrix} \lambda_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_{2} &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 0 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda_{n} \end{bmatrix} \begin{bmatrix} \mathbf{u}_{1}^{T} \\ \mathbf{u}_{2}^{T} \\ \vdots \\ \mathbf{u}_{n}^{T} \end{bmatrix} \\
    &amp; = \begin{bmatrix} \lambda_{1}\mathbf{u}_{1} &amp; \lambda_{2}\mathbf{u}_{2} &amp; \cdots &amp; \lambda_{n}\mathbf{u}_{n} \end{bmatrix} \begin{bmatrix} \mathbf{u}_{1}^{T} \\ \mathbf{u}_{2}^{T} \\ \vdots \\ \mathbf{u}_{n}^{T} \end{bmatrix}  \\
    &amp; = \lambda_{1}\mathbf{u}_{1}\mathbf{u}_{1}^{T} + \lambda_{2}\mathbf{u}_{2}\mathbf{u}_{2}^{T} + \cdots + \lambda_{n}\mathbf{u}_{n}\mathbf{u}_{n}^{T}
  \end{aligned}
\end{equation}
<p>
위 식에서 각 항 $&lambda;_{i}\mathbf{u}_{j}\mathbf{u}_{j}^{T}$은 $\mathbf{u}_{j}$에 의해 Span된 부분공간에 프로젝션된 다음 고유값 $&lambda;_{i}$만큼 스케일된 벡터로 볼 수 있다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org49a68a9" class="outline-2">
<h2 id="org49a68a9"><span class="section-number-2">61</span> Positive (Semi-)Definite Matrices</h2>
<div class="outline-text-2" id="text-61">
<p>
정방행렬 $\mathbf{A}&isin;\mathbb{R}^{n&times; n}$이 있을 때 0이 아닌 모든 벡터 $&forall; \mathbf{x}&ne; 0$에 대하여 $\mathbf{x}^{T}\mathbf{A}\mathbf{x}&gt; 0$을 만족하는 경우 $\mathbf{A}$를 <b>Positive Definite</b> 행렬이라고 한다. 만약 $\mathbf{x}^{T}\mathbf{A}\mathbf{x}&ge; 0$인 경우 <b>Positive Semi-Definite</b> 행렬이라고 한다.
</p>

<p>
정방행렬 $\mathbf{A}$가 Postivie Definite인 경우 <b>$\mathbf{A}$의 고유값은 항상 모두 양수이다.</b>
</p>

<hr />
</div>
</div>
<div id="outline-container-orgc6ee40c" class="outline-2">
<h2 id="orgc6ee40c"><span class="section-number-2">62</span> Symmetric Positive Definite Matrices</h2>
<div class="outline-text-2" id="text-62">
<p>
행렬 $\mathbf{S}&isin; \mathbb{R}^{n&times; n}$이 대칭이면서 Positive Definite인 경우 Spectral Decomposition의 모든 고유값은 항상 양수가 된다.
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{S} &amp; = \mathbf{UDU}^{-1} = \mathbf{UDU}^{T} = \begin{bmatrix} \mathbf{u}_{1} &amp; \mathbf{u}_{2} &amp; \cdots &amp; \mathbf{u}_{n} \end{bmatrix}  \begin{bmatrix} \lambda_{1} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; \lambda_{2} &amp; \ddots &amp; \vdots \\ \vdots &amp; \ddots &amp; \ddots &amp; 0 \\ 0 &amp; \cdots &amp; 0 &amp; \lambda_{n} \end{bmatrix} \begin{bmatrix} \mathbf{u}_{1}^{T} \\ \mathbf{u}_{2}^{T} \\ \vdots \\ \mathbf{u}_{n}^{T} \end{bmatrix} \\
    &amp; = \lambda_{1}\mathbf{u}_{1}\mathbf{u}_{1}^{T} + \lambda_{2}\mathbf{u}_{2}\mathbf{u}_{2}^{T} + \cdots + \lambda_{n}\mathbf{u}_{n}\mathbf{u}_{n}^{T} \\
    &amp; \text{where, } \lambda_{j} &gt; 0, \forall j = 1,\cdots,n
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-orgab500f3" class="outline-2">
<h2 id="orgab500f3"><span class="section-number-2">63</span> Back to Computing SVD</h2>
<div class="outline-text-2" id="text-63">
<p>
행렬 $\mathbf{A}$에 대하여 $\mathbf{AA}^{T} = \mathbf{A}^{T}\mathbf{A}= \mathbf{S}$인 대칭행렬이 존재할 때 $\mathbf{S}$가 Positive (Semi-)Definite한 경우
</p>

\begin{equation}
  \begin{aligned}
    &amp; \mathbf{x}^{T}\mathbf{AA}^{T}\mathbf{x} = (\mathbf{A}^{T}\mathbf{x})^{T}(\mathbf{A}^{T}\mathbf{x}) = \| \mathbf{A}^{T}\mathbf{x} \| \ge 0 \\
    &amp; \mathbf{x}^{T}\mathbf{A}^{T}\mathbf{Ax} = (\mathbf{Ax})^{T}(\mathbf{Ax}) = \| \mathbf{Ax} \|^{2} \ge 0
  \end{aligned}
\end{equation}
<p>
즉, $\mathbf{AA}^{T} = \mathbf{U}&Sigma;^{2}\mathbf{U}^{T}$와 $\mathbf{A}^{T}\mathbf{A}= \mathbf{V}&Sigma;^{2}\mathbf{V}^{T}$에서 $&Sigma;^{2}$의 값은 항상 양수가 된다.
</p>

<p>
임의의 직각 행렬 $\mathbf{A}&isin; \mathbb{R}^{m&times; n}$에 대하여 특이값 분해는 언제나 존재한다. $\mathbf{A} \in \mathbb{R}^{n\times n}$행렬의 경우 고유값 분해가 존재하지 않을 수 있지만 특이값 분해는 항상 존재한다. 대칭이면서 동시에 Positive Definite인 정방 행렬 $\mathbf{S}&isin; \mathbb{R}^{n&times; n}$은 항상 고유값 분해값이 존재하며 이는 특이값 분해와 동일하다.
</p>

<hr />
</div>
</div>
<div id="outline-container-org98db3a4" class="outline-2">
<h2 id="org98db3a4"><span class="section-number-2">64</span> Eigendecomposition in Machine Learning</h2>
<div class="outline-text-2" id="text-64">
<p>
일반적으로 머신러닝에서는 대칭이고 Positive Definite인 행렬을 다룬다. 예를 들면, $\mathbf{A}&isin; \mathbb{R}^{10 &times; 3}$인 행렬이 있고 각 열은 사람을 의미하고 각 행은 Feature를 의미한다고 가정했을 때, <b>$\mathbf{A}^{T}\mathbf{A}&isin; \mathbb{R}^{3&times; 3}$는 각 사람들 간 유사도</b> 를 의미하고 <b>$\mathbf{AA}^{T} &isin; \mathbb{R}^{10&times; 10}$는 각 Feature들의 상관관계를 의미한다.</b> 이 때, $\mathbf{AA}^{T}$는 주성분분석(Principal Component Analysis)에서 Covariance Matrix를 구할 때 사용된다.
</p>

<hr />
</div>
</div>
<div id="outline-container-orgdc96805" class="outline-2">
<h2 id="orgdc96805"><span class="section-number-2">65</span> Low Rank Approximation of a Matrix</h2>
<div class="outline-text-2" id="text-65">
<p>
행렬 $\mathbf{A}&isin; \mathbb{R}^{m&times; n}$이 주어졌을 때 예를 들어, $\mathbf{A}$의 원래 rank가 r일 때, 행렬 $\mathbf{A}$에서 rank를 r 이하를 가진 근사행렬 $\hat{\mathbf{A}}$을 찾는 Low Rank Approximation을 수행할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    &amp; \hat{\mathbf{A}}_{r} = \arg\min_{\hat{\mathbf{A}}_{r}} \| \mathbf{A} - \mathbf{A}_{r} \|_{F}, \ \ \text{subject to rank} \mathbf{A}_{r} \le r \\
    &amp; \hat{\mathbf{A}}_{r} = \sum^{r}_{i=1}\sigma_{i}\mathbf{u}_{i}\mathbf{v}_{i}^{T} \quad \text{where, } \sigma_{1} \ge \sigma_{2} \ge \cdots \ge \sigma_{r}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-orgc04c417" class="outline-2">
<h2 id="orgc04c417"><span class="section-number-2">66</span> Dimension Reducing Transformation</h2>
<div class="outline-text-2" id="text-66">
<p>
Feature-by-data item 행렬 $\mathbf{A}&isin; \mathbb{R}^{m&times; n}$이 주어졌을 때 $\mathbf{G}&isin; \mathbb{R}^{m&times; r}, r &lt; m$인 변환 $\mathbf{G}^{T}: \mathbf{x}&isin; \mathbb{R}^{m} \mapsto\mathbf{y}&isin; \mathbb{R}^{r}$을 생각해보면
</p>

\begin{equation}
  \begin{aligned}
    \mathbf{y}_{i} = \mathbf{G}^{T}\mathbf{a}_{i}
  \end{aligned}
\end{equation}
<p>
가 성립하고 $\mathbf{G}$의 각 열들은 정규직교벡터이며 데이터의 유사도 행렬 $\mathbf{S}= \mathbf{A}^{T}\mathbf{A}$의 유사도를 보존하는 <b>변환 $\mathbf{G}$를 차원 축소 변환(Dimension-Reducing Transformation)</b> 이라고 한다. 
</p>

\begin{equation}
  \begin{aligned}
    &amp; \mathbf{Y} = \mathbf{G}^{T}\mathbf{A} \\
    &amp; \mathbf{Y}^{T}\mathbf{Y} = (\mathbf{G}^{T}\mathbf{A})^{T}\mathbf{G}^{T}\mathbf{A}  = \mathbf{A}^{T}\mathbf{GG}^{T}\mathbf{A}
  \end{aligned}
\end{equation}
<p>
이 때 차원축소변환 $\hat{\mathbf{G}}$은 다음과 같이 추정할 수 있다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{G}} = \arg\min_{\mathbf{G}} \| S - \mathbf{A}^{T}\mathbf{GG}^{T}\mathbf{A} \|_{F} \ \ \text{subject to } \mathbf{G}^{T}\mathbf{G} = \mathbf{I}_{k}
  \end{aligned}
\end{equation}
<p>
주어진 행렬 $\mathbf{A}= \mathbf{U}&Sigma; \mathbf{V}^{T} = &sum;^{n}_{i=1}&sigma;_{i}\mathbf{u}_{i}\mathbf{v}_{i}^{T}$에 대하여 최적의 해는 다음과 같다.
</p>

\begin{equation}
  \begin{aligned}
    \hat{\mathbf{G}} = \mathbf{U}_{r} = \begin{bmatrix} \mathbf{u}_{1} &amp; \mathbf{u}_{2} &amp; \cdots &amp; \mathbf{u}_{r} \end{bmatrix}
  \end{aligned}
\end{equation}
<hr />
</div>
</div>
<div id="outline-container-org0084784" class="outline-2">
<h2 id="org0084784"><span class="section-number-2">67</span> Reference</h2>
<div class="outline-text-2" id="text-67">
<p>
<a href="https://www.edwith.org/linearalgebra4ai/joinLectures/14072">edwith 인공지능을 위한 선형대수, 주재걸 교수</a>
</p>
</div>
</div>
